{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dilipharish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/dilipharish/Library/Caches/pypoetry/virtualenvs/freesound-crossmodal-search-rhICCOhS-py3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels (5225,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (5225) does not match length of index (1045)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Add the cluster labels to the DataFrame\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mCluster\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m labels\n\u001b[1;32m     71\u001b[0m \u001b[39m# Create a scatter plot with hover text\u001b[39;00m\n\u001b[1;32m     72\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mscatter(x\u001b[39m=\u001b[39mX_2d[:, \u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mX_2d[:, \u001b[39m1\u001b[39m], color\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mCluster\u001b[39m\u001b[39m'\u001b[39m], hover_data\u001b[39m=\u001b[39m[df[\u001b[39m'\u001b[39m\u001b[39mcaption_1\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mcaption_2\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mcaption_3\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mcaption_4\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mcaption_5\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/freesound-crossmodal-search-rhICCOhS-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/freesound-crossmodal-search-rhICCOhS-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4145\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/freesound-crossmodal-search-rhICCOhS-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4869\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4870\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4871\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/freesound-crossmodal-search-rhICCOhS-py3.10/lib/python3.10/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (5225) does not match length of index (1045)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import plotly.express as px\n",
    "\n",
    "# read the csv file clotho_captions_evaluation.csv in data folder \n",
    "df = pd.read_csv('/Users/dilipharish/Master-Thesis/dh-freesound-crossmodal-search/data/clotho_captions_evaluation.csv')\n",
    "\n",
    "# print('df.head()', df.head())\n",
    "# # Read the CSV file\n",
    "# df = pd.read_csv('input.csv')\n",
    "\n",
    "# Preprocess the sentences\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Join the words back into a sentence\n",
    "        preprocessed_sentence = ' '.join(words)\n",
    "        \n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Apply preprocessing to the description columns\n",
    "text_columns = ['caption_1', 'caption_2', 'caption_3', 'caption_4', 'caption_5']\n",
    "preprocessed_sentences = []\n",
    "for column in text_columns:\n",
    "    df[column] = df[column].fillna('') # fillna is used to fill the missing values with a specified value\n",
    "\n",
    "    # applies the pre-process function to each of the text columns and then concatenates the results in a single list\n",
    "    preprocessed_sentences.extend(df[column].apply(preprocess_text).sum()) \n",
    "\n",
    "# Create vector representations of sentences using TF-IDF, convert text description in to a numerical vector\n",
    "vectorizer = TfidfVectorizer()\n",
    "sentence_vectors = vectorizer.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# # Cluster the sentences using K-Means\n",
    "# num_clusters = 10  # Set the desired number of clusters\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "# kmeans.fit(sentence_vectors)\n",
    "\n",
    "# Reduce the dimensionality of the vectors for visualization\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_2d = svd.fit_transform(sentence_vectors)\n",
    "\n",
    "# Perform clustering using K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(sentence_vectors)\n",
    "\n",
    "print('labels', labels.shape)\n",
    "print('df[caption_1]', df['caption_1'].shape)\n",
    "# Add the cluster labels to the DataFrame\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Create a scatter plot with hover text\n",
    "fig = px.scatter(x=X_2d[:, 0], y=X_2d[:, 1], color=df['Cluster'], hover_data=[df['caption_1'], df['caption_2'], df['caption_3'], df['caption_4'], df['caption_5']])\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "# Set plot title and axis labels\n",
    "fig.update_layout(title='Sentence Clustering', xaxis_title='Component 1', yaxis_title='Component 2')\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()\n",
    "# # Assign cluster labels to the sentences\n",
    "# cluster_labels = kmeans.labels_\n",
    "\n",
    "# # Print the clusters and their sentences\n",
    "# for cluster_id in range(num_clusters):\n",
    "#     cluster_sentences = [preprocessed_sentences[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "    \n",
    "#     print(f\"Cluster {cluster_id}:\")\n",
    "#     for sentence in cluster_sentences:\n",
    "#         print(sentence)\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freesound-crossmodal-search-rhICCOhS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
